{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords.words('english')\n",
        "nltk.download('punkt_tab')\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "XOg2qXt4-Rgw",
        "outputId": "1371f30b-2384-45cc-df19-98e0684194cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport nltk\\nimport pandas as pd\\nimport re\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nnltk.download('stopwords')\\nnltk.download('punkt')\\nstopwords.words('english')\\nnltk.download('punkt_tab') \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Load the dataset\n",
        "file_path = '/content/true_data_1.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "lZ1d968H_kA3",
        "outputId": "e05bdcb2-9a82-4ba7-ffbd-87eb3d674e80"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Load the dataset\\nfile_path = '/content/true_data_1.csv'\\ndata = pd.read_csv(file_path)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Get the list of stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords from text\n",
        "def remove_stopwords(text_list):\n",
        "    # Convert the string representation of the list to an actual list\n",
        "    comments = eval(text_list)\n",
        "    # Process each comment\n",
        "    cleaned_comments = []\n",
        "    for comment in comments:\n",
        "        # Tokenize and remove stopwords\n",
        "        words = word_tokenize(comment)\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "        # Join words back into a string\n",
        "        cleaned_comments.append(' '.join(filtered_words))\n",
        "    return str(cleaned_comments)\n",
        "\n",
        "# Apply the function to the column\n",
        "data['cust_surv_ansr_cmnt_tx_list'] = data['cust_surv_ansr_cmnt_tx_list'].apply(remove_stopwords)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "k4De53fr-iAI",
        "outputId": "80c51b38-072a-41f0-c9c2-4c8db42df560"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Get the list of stopwords\\nstop_words = set(stopwords.words('english'))\\n\\n# Function to remove stopwords from text\\ndef remove_stopwords(text_list):\\n    # Convert the string representation of the list to an actual list\\n    comments = eval(text_list)\\n    # Process each comment\\n    cleaned_comments = []\\n    for comment in comments:\\n        # Tokenize and remove stopwords\\n        words = word_tokenize(comment)\\n        filtered_words = [word for word in words if word.lower() not in stop_words]\\n        # Join words back into a string\\n        cleaned_comments.append(' '.join(filtered_words))\\n    return str(cleaned_comments)\\n\\n# Apply the function to the column\\ndata['cust_surv_ansr_cmnt_tx_list'] = data['cust_surv_ansr_cmnt_tx_list'].apply(remove_stopwords)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uiWDyOHN9RZB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "2906fbec-3800-4c8f-d14a-2b2f0e113219"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom collections import Counter\\nimport re\\n\\n\\n\\n# Function to tokenize text into words (unigrams)\\ndef tokenize(text):\\n    return re.findall(r\\'\\x08\\\\w+\\x08\\', text.lower())\\n\\n# Generating top 10 unigrams for each value in \\'cust_surv_ansr_cmnt_tx_list\\'\\nunigram_results = {}\\nfor idx, comment_list in enumerate(data[\\'cust_surv_ansr_cmnt_tx_list_stop_words_removed\\']):\\n    # Convert string representation of list to actual list\\n    comments = eval(comment_list)  # Using eval to convert string list to actual list\\n    all_words = [word for comment in comments for word in tokenize(comment)]\\n    unigrams = Counter(all_words).most_common(10)\\n    unigram_results[f\"Row {idx}\"] = unigrams\\n\\n# Display the results\\nfor row, unigrams in unigram_results.items():\\n    print(f\"{row}: {unigrams}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "'''\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# Function to tokenize text into words (unigrams)\n",
        "def tokenize(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "# Generating top 10 unigrams for each value in 'cust_surv_ansr_cmnt_tx_list'\n",
        "unigram_results = {}\n",
        "for idx, comment_list in enumerate(data['cust_surv_ansr_cmnt_tx_list_stop_words_removed']):\n",
        "    # Convert string representation of list to actual list\n",
        "    comments = eval(comment_list)  # Using eval to convert string list to actual list\n",
        "    all_words = [word for comment in comments for word in tokenize(comment)]\n",
        "    unigrams = Counter(all_words).most_common(10)\n",
        "    unigram_results[f\"Row {idx}\"] = unigrams\n",
        "\n",
        "# Display the results\n",
        "for row, unigrams in unigram_results.items():\n",
        "    print(f\"{row}: {unigrams}\")\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Function to generate bigrams from a list of words\n",
        "def generate_bigrams(words):\n",
        "    return [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "\n",
        "# Generating top 10 bigrams for each value in 'cust_surv_ansr_cmnt_tx_list'\n",
        "bigram_results = {}\n",
        "for idx, comment_list in enumerate(data['cust_surv_ansr_cmnt_tx_list']):\n",
        "    # Convert string representation of list to actual list\n",
        "    comments = eval(comment_list)  # Using eval to convert string list to actual list\n",
        "    all_words = [word for comment in comments for word in tokenize(comment)]\n",
        "    bigrams = generate_bigrams(all_words)\n",
        "    top_bigrams = Counter(bigrams).most_common(10)\n",
        "    bigram_results[f\"Row {idx}\"] = top_bigrams\n",
        "\n",
        "# Display the results\n",
        "for row, bigrams in bigram_results.items():\n",
        "    print(f\"{row}: {bigrams}\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "CMADKimK-qvt",
        "outputId": "c85c9891-eaef-4b25-80f1-6b389d252f05"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Function to generate bigrams from a list of words\\ndef generate_bigrams(words):\\n    return [(words[i], words[i+1]) for i in range(len(words)-1)]\\n\\n# Generating top 10 bigrams for each value in \\'cust_surv_ansr_cmnt_tx_list\\'\\nbigram_results = {}\\nfor idx, comment_list in enumerate(data[\\'cust_surv_ansr_cmnt_tx_list\\']):\\n    # Convert string representation of list to actual list\\n    comments = eval(comment_list)  # Using eval to convert string list to actual list\\n    all_words = [word for comment in comments for word in tokenize(comment)]\\n    bigrams = generate_bigrams(all_words)\\n    top_bigrams = Counter(bigrams).most_common(10)\\n    bigram_results[f\"Row {idx}\"] = top_bigrams\\n\\n# Display the results\\nfor row, bigrams in bigram_results.items():\\n    print(f\"{row}: {bigrams}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Store the top 10 bigrams for each row in a new column\n",
        "data['top_10_bigrams'] = data['cust_surv_ansr_cmnt_tx_list'].apply(\n",
        "    lambda comment_list: Counter(\n",
        "        generate_bigrams(\n",
        "            [word for comment in eval(comment_list) for word in tokenize(comment)]\n",
        "        )\n",
        "    ).most_common(10)\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "ILXUWhMnAvbM",
        "outputId": "a61a3260-4d08-4e86-c481-917df8aae0fb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Store the top 10 bigrams for each row in a new column\\ndata['top_10_bigrams'] = data['cust_surv_ansr_cmnt_tx_list'].apply(\\n    lambda comment_list: Counter(\\n        generate_bigrams(\\n            [word for comment in eval(comment_list) for word in tokenize(comment)]\\n        )\\n    ).most_common(10)\\n)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Generating top 10 unigrams for each value in 'cust_surv_ansr_cmnt_tx_list'\n",
        "data['top_10_unigrams'] = data['cust_surv_ansr_cmnt_tx_list'].apply(\n",
        "    lambda comment_list: Counter(\n",
        "        generate_unigrams(\n",
        "            [word for comment in eval(comment_list) for word in tokenize(comment)]\n",
        "        )\n",
        "    ).most_common(10)\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "UR9tEzfbAwA4",
        "outputId": "80e11c93-e136-45ff-c2ab-ba9b95f1e349"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Generating top 10 unigrams for each value in 'cust_surv_ansr_cmnt_tx_list'\\ndata['top_10_unigrams'] = data['cust_surv_ansr_cmnt_tx_list'].apply(\\n    lambda comment_list: Counter(\\n        generate_unigrams(\\n            [word for comment in eval(comment_list) for word in tokenize(comment)]\\n        )\\n    ).most_common(10)\\n)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords.words('english')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/true_data_1.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Get the list of stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords from text\n",
        "def remove_stopwords(text_list):\n",
        "    # Convert the string representation of the list to an actual list\n",
        "    comments = eval(text_list)\n",
        "    # Process each comment\n",
        "    cleaned_comments = []\n",
        "    for comment in comments:\n",
        "        # Tokenize and remove stopwords\n",
        "        words = word_tokenize(comment)\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "        # Join words back into a string\n",
        "        cleaned_comments.append(' '.join(filtered_words))\n",
        "    return str(cleaned_comments)\n",
        "\n",
        "# Apply the function to the column\n",
        "data['cust_surv_ansr_cmnt_tx_list'] = data['cust_surv_ansr_cmnt_tx_list'].apply(remove_stopwords)\n",
        "\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# Function to tokenize text into words (unigrams)\n",
        "def tokenize(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "# Generating top 10 unigrams for each value in 'cust_surv_ansr_cmnt_tx_list'\n",
        "unigram_results = {}\n",
        "# Assuming 'cust_surv_ansr_cmnt_tx_list' column exists after stop word removal\n",
        "for idx, comment_list in enumerate(data['cust_surv_ansr_cmnt_tx_list']):\n",
        "    # Convert string representation of list to actual list\n",
        "    comments = eval(comment_list)  # Using eval to convert string list to actual list\n",
        "    all_words = [word for comment in comments for word in tokenize(comment)]\n",
        "    unigrams = Counter(all_words).most_common(10)\n",
        "    unigram_results[f\"Row {idx}\"] = unigrams\n",
        "\n",
        "# Display the results\n",
        "for row, unigrams in unigram_results.items():\n",
        "    print(f\"{row}: {unigrams}\")\n",
        "\n",
        "\n",
        "# Function to generate bigrams from a list of words\n",
        "def generate_bigrams(words):\n",
        "    return [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "\n",
        "# Generating top 10 bigrams for each value in 'cust_surv_ansr_cmnt_tx_list'\n",
        "bigram_results = {}\n",
        "for idx, comment_list in enumerate(data['cust_surv_ansr_cmnt_tx_list']):\n",
        "    # Convert string representation of list to actual list\n",
        "    comments = eval(comment_list)  # Using eval to convert string list to actual list\n",
        "    all_words = [word for comment in comments for word in tokenize(comment)]\n",
        "    bigrams = generate_bigrams(all_words)\n",
        "    top_bigrams = Counter(bigrams).most_common(10)\n",
        "    bigram_results[f\"Row {idx}\"] = top_bigrams\n",
        "\n",
        "# Display the results\n",
        "for row, bigrams in bigram_results.items():\n",
        "    print(f\"{row}: {bigrams}\")\n",
        "\n",
        "# Store the top 10 bigrams for each row in a new column\n",
        "data['top_10_bigrams'] = data['cust_surv_ansr_cmnt_tx_list'].apply(\n",
        "    lambda comment_list: Counter(\n",
        "        generate_bigrams(\n",
        "            [word for comment in eval(comment_list) for word in tokenize(comment)]\n",
        "        )\n",
        "    ).most_common(10)\n",
        ")\n",
        "\n",
        "# Generating top 10 unigrams for each value in 'cust_surv_ansr_cmnt_tx_list'\n",
        "data['top_10_unigrams'] = data['cust_surv_ansr_cmnt_tx_list'].apply(\n",
        "    lambda comment_list: Counter(\n",
        "        # Use tokenize directly for unigrams:\n",
        "        [word for comment in eval(comment_list) for word in tokenize(comment)]\n",
        "    ).most_common(10)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKLpeRlaBOgu",
        "outputId": "2bb071bf-e3b6-4613-a603-81af71aa3584"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 0: [('nice', 51), ('food', 49), ('service', 19), ('good', 16), ('great', 11), ('job', 4), ('staff', 4), ('really', 3), ('order', 2), ('member', 2)]\n",
            "Row 1: [('excellent', 38), ('amazing', 31), ('great', 11), ('good', 4), ('restaurant', 3), ('food', 3), ('order', 3), ('service', 2), ('amna', 2), ('egg', 2)]\n",
            "Row 2: [('kind', 64), ('delicious', 64), ('food', 57), ('polite', 46), ('everyone', 45), ('restaurant', 45), ('everything', 44), ('fast', 43), ('tidy', 42), ('experience', 27)]\n",
            "Row 3: [('cam', 169), ('tasty', 160), ('service', 19), ('good', 11), ('tastt', 9), ('order', 6), ('great', 5), ('food', 5), ('staff', 4), ('screen', 3)]\n",
            "Row 4: [('service', 48), ('good', 34), ('excellent', 11), ('great', 5), ('friendly', 3), ('staff', 3), ('food', 2), ('hygenic', 1), ('kal', 1), ('insured', 1)]\n",
            "Row 5: [('good', 76), ('food', 63), ('service', 17), ('excellent', 3), ('great', 3), ('warm', 2), ('price', 1), ('fpod', 1), ('verrry', 1), ('love', 1)]\n",
            "Row 6: [('excellent', 3), ('good', 1)]\n",
            "Row 7: [('service', 64), ('good', 36), ('great', 21), ('staff', 10), ('helpful', 6), ('friendly', 4), ('excellent', 4), ('speed', 3), ('speedy', 2), ('one', 2)]\n",
            "Row 0: [(('nice', 'food'), 48), (('food', 'nice'), 33), (('good', 'service'), 9), (('service', 'nice'), 9), (('food', 'good'), 8), (('great', 'service'), 8), (('food', 'great'), 6), (('job', 'nice'), 3), (('great', 'job'), 3), (('service', 'great'), 3)]\n",
            "Row 1: [(('excellent', 'amazing'), 18), (('amazing', 'excellent'), 15), (('excellent', 'excellent'), 9), (('amazing', 'amazing'), 6), (('great', 'excellent'), 5), (('excellent', 'great'), 4), (('great', 'amazing'), 4), (('amazing', 'great'), 4), (('amazing', 'good'), 2), (('good', 'excellent'), 2)]\n",
            "Row 2: [(('food', 'delicious'), 43), (('polite', 'food'), 23), (('everyone', 'amazing'), 22), (('amazing', 'polite'), 22), (('polite', 'kind'), 22), (('kind', 'rushing'), 22), (('rushing', 'round'), 22), (('round', 'polite'), 22), (('delicious', 'fast'), 22), (('fast', 'everyone'), 21)]\n",
            "Row 3: [(('cam', 'tasty'), 157), (('tasty', 'cam'), 145), (('cam', 'tastt'), 9), (('tastt', 'cam'), 9), (('good', 'service'), 5), (('service', 'good'), 4), (('service', 'cam'), 4), (('abdul', 'haleem'), 3), (('great', 'service'), 3), (('great', 'food'), 2)]\n",
            "Row 4: [(('good', 'service'), 30), (('service', 'good'), 26), (('excellent', 'service'), 11), (('service', 'excellent'), 9), (('great', 'service'), 5), (('service', 'great'), 4), (('staff', 'good'), 3), (('good', 'good'), 2), (('service', 'friendly'), 2), (('friendly', 'staff'), 2)]\n",
            "Row 5: [(('good', 'food'), 57), (('food', 'good'), 51), (('good', 'service'), 10), (('service', 'good'), 10), (('good', 'good'), 5), (('excellent', 'service'), 2), (('great', 'service'), 2), (('service', 'excellent'), 2), (('warm', 'food'), 2), (('good', 'price'), 1)]\n",
            "Row 6: [(('excellent', 'excellent'), 1), (('excellent', 'good'), 1), (('good', 'excellent'), 1)]\n",
            "Row 7: [(('good', 'service'), 35), (('service', 'good'), 27), (('great', 'service'), 19), (('service', 'great'), 18), (('helpful', 'staff'), 5), (('service', 'helpful'), 4), (('speed', 'service'), 3), (('service', 'friendly'), 3), (('friendly', 'staff'), 3), (('staff', 'good'), 3)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNtTZcdQC2fk",
        "outputId": "d6a51fff-867e-41cf-9e89-eecb82a70f12"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sIjU-4wC4fk",
        "outputId": "6bb035db-1078-416a-a6f5-5e9c5fb22d73"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['mcd_gbal_lcat_id_nu', 'cust_surv_ansr_cmnt_tx_list',\n",
              "       'cust_surv_rsnd_id_list', 'len_cust_surv_ansr_cmnt_tx_list',\n",
              "       'len_cust_surv_rsnd_id_list', 'Avg_Word_Length', 'Avg_Sentence_Length',\n",
              "       'Type_Token_Ratio', 'Flesch_Reading_Ease', 'Function_Word_Percentage',\n",
              "       'Punctuation_Per_Word', 'similarity_score', 'Anomaly', 'month',\n",
              "       'cust_surv_ansr_cmnt_tx_list_stop_words_removed', 'top_10_bigrams',\n",
              "       'top_10_unigrams'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('/content/true_data_1_updated.csv',index = False)"
      ],
      "metadata": {
        "id": "faJvl_2PC6Gg"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}